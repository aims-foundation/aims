<!-- ---
# title: "Multidimensional Construct"
format: html
jupyter: python3
execute:
  echo: true      # show code
  eval: false      # actually run it 
  engine: pyodide
  pyodide:
    auto: true
--- -->

---
# title: "Multidimensional Construct"
format: html
filters:
  - pyodide
execute:
  eval: true
  engine: pyodide
  cache: false
  pyodide:
    auto: true
---



<!--
\item Truong et al. Beyond Mean Score: Factor Model for Reliable and Efficient AI Evaluation.
\item Truong et al. Measuring Without Judging: Reliable \& Efficient Evaluation via Factor Models % <- Preference-based Evaluation with the factor model
% \item Truong et al. In Searching for the Nomological Network of AI Capabilities with Latent Variable Models -- with Mike
-->

![Correlation structure within benchmarks reveals hidden heterogeneity. (a) We show the response matrix for a subset of models and items, decomposed by factor analysis into model factors (U), item factors (V), and intercepts (Z). (b) We show item-item tetrachoric correlations for two benchmarks, Open LLM Math and MUSR. Below each correlation matrix, we present the structure matrix (S) obtained via factor analysis, along with the cluster-level correlation matrix that summarizes the relationships between cluster mean scores and the overall mean score](Figures/Fig1.png){#fig-fa_main width="100%"}



# Measurement of Multidimentional Construct  {#sec-construct}
A theory of what the latent constructs are, and how they connect.     

Factor modeling allows us to move beyond one-dimensional capability scores and examine the *multidimensional construct structure* underlying model behavior. Each factor represents a latent capability dimension inferred from the joint response matrix $Y$, while the **structure matrix**  $S$ quantifies how strongly each item loads on each latent dimension. 

A key issue with using mean score as headline number on benchmark leaderboards is that
benchmarks are rarely homogeneous. Intentionally or not, they often combine items that test different capabilities, and even a single benchmark item may test a combination of capabilities. Two
models with identical mean scores may thus excel on different capability dimensions. For example, one model might be strong in reasoning but weak in factual recall, while another may have the
reverse capability profile. Indeed, in our analysis we find that response correctness across large subsets of questions from a given benchmark can be highly negatively correlated (see [@fig-fa_main]).


## Introduce to the Dataset 

In this section, we explore the Open LLM dataset, which forms the empirical foundation for our factor analysis. This dataset aggregates evaluation results across multiple benchmarks, allowing us to study latent capability dimensions among large language models.

The dataset contains 4,416 test takers (models) and 21,176 items (benchmark questions). Each entry in the response matrix indicates whether a model answered a particular question correctly (True) or incorrectly (False). By linking these binary responses with metadata for both models and items, we can analyze multidimensional performance structures across benchmarks.

### Load library and Set Seed

Before we begin, we import the necessary dependencies and set a random seed for reproducibility.
This ensures that all subsequent computations—especially those involving stochastic optimization—are deterministic.

```{python}

import torch
from torch.optim import LBFGS
import torch.nn as nn
from torch.distributions import Bernoulli
from tqdm import tqdm
from torchmetrics import AUROC
import random
import numpy as np
import pandas as pd
import pickle
import torch.nn.functional as F
import matplotlib.pyplot as plt
import time
from huggingface_hub import snapshot_download

seed = 0
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)
```

### A Brief Overview of the Curated Dataset

The dataset is hosted on the Hugging Face Hub under the repository stair-lab/reeval_fa. It includes the joint binary response matrix as well as supplementary metadata for each model (“test taker”) and each benchmark item (“question”). Let’s begin by downloading and loading the response matrix.

#### The response matrix

```{python}

local_path = snapshot_download(repo_id="stair-lab/reeval_fa", repo_type="dataset")
with open(f"{local_path}/data/benchmark_data_open_llm_full_no_arc.pkl", "rb") as f:
    open_llm_response_dataset = pickle.load(f)


print(open_llm_response_dataset.shape)
print(open_llm_response_dataset.iloc[:5,:2])
```

The output shows that the dataset contains 4,416 rows and 21,176 columns—each row corresponds to a model, and each column corresponds to an evaluation item. Every cell stores a Boolean value representing whether that model produced the correct response to that item. We can think of this as a large model–item matrix, analogous to a psychometric test matrix where models are test takers and questions are items.”

#### Enriching Model Information
The DataFrame index contains model names such as "01-ai/Yi-1.5-34B".
To enrich this information (e.g., model family, precision type, or training configuration), we merge with a metadata table.

```{python}
print(open_llm_response_dataset.index[:5])
```

Now, let’s join these identifiers with the full metadata file:

```{python}
model_meta_info = pd.read_csv(f"{local_path}/data/openllm_all_model_info_full.csv")
model_meta_info = model_meta_info.drop_duplicates(subset=["fullname"], keep="last")

result_model_name = open_llm_response_dataset.reset_index().rename(columns={"index": "fullname"})[['fullname']]
model_meta_info = pd.merge(result_model_name, model_meta_info, on="fullname", how="left")


print("Preview of merged model metadata (first 5 rows):")
print(model_meta_info.iloc[:5, :2].to_string(index=False))

print("\nColumn names (first 5):")
print(", ".join(model_meta_info.columns[:5]))

```

This enriched table lets us analyze performance across various model-level factors—such as model family (e.g., LLaMA vs. Yi), size (e.g., 7B vs. 34B), or numerical precision (e.g., FP16 vs. BF16).


#### Retrieving Original Item Information

Next, we turn to the columns of the response matrix.
Each column is a tuple indicating the benchmark and the specific item identifier

```{python}
print("First 5 columns in the response dataset:")
print(open_llm_response_dataset.columns[:10])

```

To see this in practice, let’s examine one specific item—in this case, the 2026th question in the dataset:

```{python}

# Select the 2026th item reference (column name)
target_item_reference = open_llm_response_dataset.columns[2026]
print(target_item_reference)

```

We can separate the benchmark name and the index as follows:

```{python}

# Extract benchmark name and item index
target_item_benchmark = target_item_reference[0]
target_item_index = int(target_item_reference[1][len(target_item_benchmark) + 1:])

print(target_item_benchmark, target_item_index)


```

The output shows which benchmark the item belongs to (for example, mmlu_pro) and its internal index within that benchmark. We can then retrieve the full item content—including question text, answer options, and metadata—from the pre-stored dataset:

```{python}

# Retrieve the question content from the pre-stored dictionary
with open(f"{local_path}/data/all_questions.pkl", "rb") as f:
    all_questions = pickle.load(f)

retrieved_question = all_questions[target_item_benchmark][target_item_index]

print(retrieved_question)

```

In the case of MMLU-PRO, each entry includes the question text, answer options, correct answer, category, and benchmark source.
The curated item information remains exactly as in the original datasets—unaltered in structure or content—though the schema can differ across benchmarks.


#### Reconstructing the Original Model Input

The Open LLM Benchmark follows the LM-Harness evaluation framework. To reproduce the actual text input presented to models during evaluation, we implement helper functions that replicates LM-Harness’s prompt-construction template. Each function below corresponds to one benchmark’s data format.


```{python}

import ast
import string
DOC_TO_TEXT = "{narrative}\n\n" "{question}\n\n" "{choices}\n" "Answer:"

def doc_to_text_musr(doc):
    """
    Convert a doc to text.
    """
    choices = ""
    for i, choice in enumerate(ast.literal_eval(doc["choices"])):
        choices += f"{i+1} - {choice}\n"

    text = DOC_TO_TEXT.format(
        narrative=doc["narrative"], question=doc["question"], choices=choices
    )

    return text

def doc_to_text_math(doc: dict) -> str:
    return "Problem:" + "\n" + doc["problem"] + "\n\n" + "Solution:"

def doc_to_text_gpqa(doc: dict) -> str:
    return f"What is the correct answer to this question: {doc['Question']}\nChoices:\n(A) {doc['choice1']}\n(B) {doc['choice2']}\n(C) {doc['choice3']}\n(D) {doc['choice4']}\nAnswer: "

def doc_to_text_mmlu_pro(doc):
    doc_to_text = f"{doc['question']}\n"

    for i in range(len(doc["options"])):
        doc_to_text += f"{string.ascii_uppercase[i]}. {doc['options'][i]}\n"

    doc_to_text += "Answer:"
    return doc_to_text

def sel_question(question,benchmark):
    if benchmark == 'ifeval':
        return question['prompt']
    elif benchmark == 'openllm_math':
        return doc_to_text_math(question)
    elif benchmark == 'musr':
        return doc_to_text_musr(question)
    elif benchmark == 'mmlu_pro':
        return doc_to_text_mmlu_pro(question)
    elif benchmark == 'bbh':
        return f"Q: {question['input']}\n\nA: '"
    elif benchmark == 'gpqa':
        return doc_to_text_gpqa(question)
    else:
        assert False

```

We can now reconstruct the exact input text for our earlier example item:


```{python}


actual_input_2026 = sel_question(retrieved_question, target_item_benchmark)
print(actual_input_2026)

```

This step ensures that our dataset’s text representation faithfully reproduces the original prompts passed to each model, and can later enable interpretation tasks.


### Processing the Data for Model Fitting

For illustration, we will focus on the Open LLM Math benchmark as a running example. We first isolate the subset of columns corresponding to this benchmark:

```{python}
sel_benchmark = "openllm_math"
col_scenarios = pd.Series([i[0] for i in open_llm_response_dataset.columns])
keep_col_sel = col_scenarios == sel_benchmark
open_llm_response_dataset = open_llm_response_dataset.loc[:, keep_col_sel.values]
print(open_llm_response_dataset.shape)
```

Next, we remove items that are too easy or too difficult—that is, items that nearly all models either solved or failed. This filtering helps stabilize downstream factor estimation.

```{python}
no_extreme_question = ((open_llm_response_dataset.mean(axis=0, skipna=True) >0.01).values) & ((open_llm_response_dataset.mean(axis=0, skipna=True) <0.99).values)
open_llm_response_dataset = open_llm_response_dataset.loc[:, no_extreme_question]
print(open_llm_response_dataset.shape)
```

Finally, we convert the DataFrame into PyTorch tensors for model fitting.


```{python}
data_withnan = torch.tensor(open_llm_response_dataset.astype("boolean").astype(float).to_numpy())
data_withneg1 = data_withnan.nan_to_num(nan=-1.0)
data_idtor = (data_withneg1 != -1).to(float)
data_with0 = data_withneg1 * data_idtor # -1 -> 0

```

### Visualizing the Response Matrix


To build intuition about the dataset, we visualize the response matrix sorted by model ability (mean score per model) and item difficulty (mean score per question).


```{python}


#%% Visualization of Model × Question Responses (Rasch-style aesthetic)
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# -------------------------------------------------------------------
# Configure global style
# -------------------------------------------------------------------
plt.rcParams.update({
    "font.size": 14,
    "axes.labelsize": 16,
    "xtick.labelsize": 13,
    "ytick.labelsize": 13,
    "legend.fontsize": 14,
    "axes.titlesize": 16,
    "figure.figsize": (8, 7),
    "figure.autolayout": True,
    "axes.edgecolor": "#444444",
    "axes.linewidth": 1.0,
})

# -------------------------------------------------------------------
# Prepare data
# -------------------------------------------------------------------
data_np = data_withneg1.cpu().numpy().astype(float)
data_np[data_np == -1] = np.nan  # treat missing as NaN

row_mean = np.nanmean(data_np, axis=1)
col_mean = np.nanmean(data_np, axis=0)

# sort by model ability (row mean) and item difficulty (col mean)
row_order = np.argsort(row_mean)
col_order = np.argsort(col_mean)
data_sorted = data_np[row_order, :][:, col_order]

# -------------------------------------------------------------------
# Build RGB image with white for NaN, coolwarm for responses
# -------------------------------------------------------------------
H, W = data_sorted.shape
rgb_image = np.ones((H, W, 3))  # white background

# Use same color scheme as Rasch example
cmap = plt.get_cmap("coolwarm")
red = np.array(cmap(1.0)[:3])   # incorrect
blue = np.array(cmap(0.0)[:3])  # correct

mask_correct = data_sorted == 1
mask_incorrect = data_sorted == 0

rgb_image[mask_correct] = blue
rgb_image[mask_incorrect] = red

# -------------------------------------------------------------------
# Plot
# -------------------------------------------------------------------
fig, ax = plt.subplots()
im = ax.imshow(rgb_image, aspect='auto')
ax.set_title("Model × Question Response Matrix (sorted by mean score)")
ax.set_xlabel("Questions (low → high mean score)")
ax.set_ylabel("Models (low → high mean score)")

# Clean look
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.tick_params(length=0)

plt.tight_layout()
plt.show()


```

The resulting figure provides an intuitive overview of the data’s structure—an essential first step before fitting latent factor models. The pattern also indicates this is a low-rank matrix

## Masking


In practical experiments, we often need to divide our dataset into training and testing parts. Here, we demonstrate a simple random masking procedure that hides 20% of the data for evaluation.
The idea is straightforward: given a binary indicator matrix data_idtor marking observed entries, we randomly sample a subset for training and reserve the remainder for testing.

<!-- 
::: {.callout-note title="code"}
```{pyodide-python}
def random_mask(data_idtor, pct = 0.8):

    train_idtor = torch.bernoulli(data_idtor * pct).int()
    test_idtor = data_idtor - train_idtor
    return train_idtor, test_idtor
train_idtor, test_idtor = random_mask(data_idtor)

device = "cpu"

Y = data_with0.to(device)
train_idtor = train_idtor.bool().to(device)
test_idtor = test_idtor.bool().to(device)


```
::: -->


```{python}
def random_mask(data_idtor, pct = 0.8):

    train_idtor = torch.bernoulli(data_idtor * pct).int()
    test_idtor = data_idtor - train_idtor
    return train_idtor, test_idtor
train_idtor, test_idtor = random_mask(data_idtor)

device = "cpu"

Y = data_with0.to(device)
train_idtor = train_idtor.bool().to(device)
test_idtor = test_idtor.bool().to(device)
```


Here we use `torch.bernoulli()` to draw binary samples with probability pct and store the resulting training and testing indicators as boolean tensors for later masking.


## Model Initialization
We fit a **Logistic Factor Model**, where each model $i$ and question $j$ are represented by $K$-dimensional latent vectors $U_i$ and $V_j$. Each question also has a bias term $Z_j$ capturing its inherent difficulty.

$$
p(Y_{ij} = 1 \mid U_i, V_j, Z_j) = \sigma(U_i^\top V_j + Z_j)
$$


<!-- 
::: {.callout-note title="code"}
```{pyodide-python}

class LogisticFM(nn.Module):
    def __init__(self, N, M, K):
        super().__init__()
        self.U = nn.Parameter(torch.randn(N, K))
        self.V = nn.Parameter(torch.randn(M, K))
        self.Z = nn.Parameter(torch.randn(M, 1))

    def forward(self):
        return torch.sigmoid(self.U @ self.V.T + self.Z.T)
        # return self.U @ self.V.T + self.Z.T



N, M = Y.shape
K = 2
eps = 1e-8
model = LogisticFM(N, M, K).to(device)
print(model)


```
::: -->

Implemented in Pytorch as below

```{python}

class LogisticFM(nn.Module):
    def __init__(self, N, M, K):
        super().__init__()
        self.U = nn.Parameter(torch.randn(N, K))
        self.V = nn.Parameter(torch.randn(M, K))
        self.Z = nn.Parameter(torch.randn(M, 1))

    def forward(self):
        return torch.sigmoid(self.U @ self.V.T + self.Z.T)
        # return self.U @ self.V.T + self.Z.T



N, M = Y.shape
K = 2
eps = 1e-8
model = LogisticFM(N, M, K).to(device)
print(model)
```

::: {.callout-note}
**Interpretation:**  
- $U_i$: latent ability of model \(i\)  
- $V_j$: latent property or “dimension” of question \(j\)  
- $Z_j$: overall question bias  
- $\sigma$: sigmoid function ensuring probabilities in \([0,1]\)
:::


## Training via LBFGS Optimization
We train the model by minimizing the Bernoulli negative log-likelihood, equivalent to binary cross-entropy.
While our full experiments use early stopping and GPU acceleration, this demonstration limits to 20 iterations on CPU for clarity.


<!-- ::: {.callout-note title="code"}
```{pyodide-python}
#| eval: false
opt = LBFGS(
    model.parameters(),
    lr=0.1,
    max_iter=20,
    history_size=10,
    line_search_fn="strong_wolfe"
)

def closure():
    # print("closure")
    t0 = time.time()
    opt.zero_grad()
    probs = model()
    # logits = model()
    t1 = time.time()
    # loss = -Bernoulli(probs=probs[train_idtor]).log_prob(Y[train_idtor].float()).mean()
    loss = F.binary_cross_entropy(probs[train_idtor], Y[train_idtor].float())
    # loss = F.binary_cross_entropy_with_logits(logits[train_idtor], Y[train_idtor].float())
    t2 = time.time()
    loss.backward()
    t3 = time.time()
    # print(t3-t2, t2-t1, t1-t0)

    return loss
# step_num = 10_000
step_num = 5
pbar = tqdm(range(step_num))

patience_step = 10
for iteration in pbar:
    loss = opt.step(closure)

```
:::

 -->


```{python}

# seed = 0
# random.seed(seed)
# np.random.seed(seed)
# torch.manual_seed(seed)
# if torch.cuda.is_available():
#     torch.cuda.manual_seed_all(seed)



opt = LBFGS(
    model.parameters(),
    lr=0.1,
    max_iter=20,
    history_size=10,
    line_search_fn="strong_wolfe"
)

def closure():
    # print("closure")
    t0 = time.time()
    opt.zero_grad()
    probs = model()
    # logits = model()
    t1 = time.time()
    # loss = -Bernoulli(probs=probs[train_idtor]).log_prob(Y[train_idtor].float()).mean()
    loss = F.binary_cross_entropy(probs[train_idtor], Y[train_idtor].float())
    # loss = F.binary_cross_entropy_with_logits(logits[train_idtor], Y[train_idtor].float())
    t2 = time.time()
    loss.backward()
    t3 = time.time()
    # print(t3-t2, t2-t1, t1-t0)

    return loss
# step_num = 10_000
step_num = 20
pbar = tqdm(range(step_num))
loss_history = []
patience_step = 10
for iteration in pbar:
    loss = opt.step(closure)
    loss_history.append(loss.item())



plt.plot(loss_history)
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.title("Training Loss History")
plt.show()        


```

The LBFGS optimizer repeatedly calls the `closure()` function to evaluate the objective and its gradient, ensuring stable convergence for this smooth, low-dimensional optimization problem.


## Evaluation

After training, we assess generalization using the held-out responses in the test mask. We compute the Area Under the ROC Curve (AUC) to measure how well the model discriminates between correct and incorrect outcomes.


<!-- ::: {.callout-note title="code"}
```{pyodide-python}

P_hat = model().detach().cpu()

auroc = AUROC(task="binary")
test_auc = auroc(P_hat.cpu()[test_idtor.cpu()], Y.cpu()[test_idtor.cpu()])

print(f"Test AUROC: {test_auc.item():.4f}")



```
:::
 -->



```{python}

P_hat = model().detach().cpu()

auroc = AUROC(task="binary")
test_auc = auroc(P_hat.cpu()[test_idtor.cpu()], Y.cpu()[test_idtor.cpu()])

print(f"Test AUROC: {test_auc.item():.4f}")

``` 
> auc of 0.91 suggest it has a strong prediction power.

## Identifying benchmark subgroups based on structure matrix
After fitting the factor model to each benchmark, we obtain estimated item loadings $\hat V_j$ and construct correlations $\Sigma$. We first standardize the estimated parameters and compute the structure matrix. The resulting structure matrix provides an interpretable representation of the relationship between items and latent factors. Clustering the rows of \(S\) with Gaussian mixture models (GMM) partitions items into statistically coherent groups, each corresponding to an emergent construct. This approach is analogous to exploratory factor analysis in psychometrics, revealing whether benchmarks are essentially unidimensional or composed of multiple, potentially antagonistic, latent skills.

### Standardization, Whitening, and Rotation

Before comparing loadings across benchmarks, we must ensure parameters are **identified** — since the decomposition is only defined up to a linear transformation.  
We apply **centering**, **whitening**, and **rotation** to align factors while preserving the likelihood.

::: {.callout-note title="Mathematical Background: Identification and Whitening"}
Because the factor decomposition is only defined up to a linear transformation, parameter identification is achieved through **centering**, **whitening**, and **rotation** constraints.

Specifically, the estimated parameters $(\hat U_\text{raw}, \hat V_\text{raw}, \hat Z_\text{raw})$ are standardized so that the factor has zero mean and unit covariance:
$$
\begin{aligned}
&\hat U_c = \hat U_\text{raw} - \bar U
\quad \implies \quad
\hat Z_c = \hat Z_\text{raw} + \hat V_\text{raw} \bar U^\top, \\[4pt]
&\hat U_w = \hat U_c L^{-\top}
\quad \implies \quad
\hat V_w = \hat V_\text{raw} L, \quad
\hat Z_w = \hat Z_c,
\end{aligned}
$$
where  
$\bar U = \frac{1}{N} \sum_i \hat U_{\text{raw}, i}$ and  
$\hat \Sigma = \frac{1}{N-1}\hat U_c^\top \hat U_c = L L^\top.$

:::

Below is the implementation of the whitening and rotation step using **Promax rotation** for interpretability.

```{python}


from sklearn.mixture import GaussianMixture
import numpy as np
from factor_analyzer import Rotator
import math

def standardize_V_Z_U_promax(U, V, Z, is_shuffle=False):
    k = U.shape[1]
    if k <= 1:
        return V, Z, U
    # whiten V
    eps = 1e-8
    I_K = torch.eye(k, device=U.device, dtype=U.dtype)
    
    # (1) Center U and shift Z: Z <- Z + V @ mean
    mean = U.mean(dim=0, keepdim=True)          # [1,K]
    U_c  = U - mean
    Z_c  = Z + (V @ mean.T)            # [M]
    assert U_c.mean(0).mean() < 1e-6
    
    # (2) Cholesky whitening on centered U
    N = U_c.shape[0]
    Sigma = (U_c.T @ U_c) / (N - 1)
    Sigma = Sigma + eps * I_K
    L = torch.linalg.cholesky(Sigma)            # Sigma = L L^T

    # Choose W = L^{-T}; then Cov(U_c W) = I
    W = torch.inverse(L).T                      # W = L^{-T}
    U_w = U_c @ W                               # [N,K]
    assert ((U_w.T @ U_w) / (U_w.shape[0]-1) - I_K).mean() < 1e-5
    assert U_w.mean(0).mean() < 1e-5
    
    # Matching V transform: V_w = V @ W^{-T} = V @ L
    V_w = V @ L                                 # [M,K]
    Z_w = Z_c                                   # unchanged
    # (3) Varimax rotation on V_w (no Kaiser normalization)
    rot = Rotator(method="promax", normalize=True, max_iter=10_000) #, max_iter=10_000
    V_r_np = rot.fit_transform(V_w.detach().cpu().numpy())   # V_r = V_w R
    R = torch.tensor(rot.rotation_, dtype=U.dtype, device=U.device)  # [K,K]
    V_r = torch.tensor(V_r_np, dtype=U.dtype, device=U.device)
    
    # IMPORTANT: rotate U with R^{-T}
    U_r = U_w @ torch.linalg.solve(R.T, I_K)    # == U_w @ R^{-T}
    Z_r = Z_w

    return V_r, Z_r, U_r


U, V, Z = model.U, model.V, model.Z

V_rot_2, Z_rot_2, U_rot_2 = standardize_V_Z_U_promax(U, V, Z)

print(V_rot_2.shape)
```

### Computing the Structure Matrix

Once the factors are standardized, we compute the **structure matrix**, which captures the correlation between each item and each latent factor.

::: {.callout-note title="Mathematical Background: Structure Matrix"}
The latent representation preserves the likelihood of $Y$ while treating its right-hand side as a **linear factor model** with additive error variance fixed at $\pi^2 / 3$.
Since $\epsilon_{ij}$ is independent of $U_i$,

$$
\begin{aligned}
Cov(Y^*_{ij}, U_{ik}) &= Cov(H_{ij}, U_{ik})
= \sum_{\ell=1}^K V_{j\ell} Cov(U_{i\ell}, U_{ik})
= (V_j^\top \Sigma)_k, \\[6pt]
Var(Y^*_{ij}) &= Var(H_{ij}) + Var(\epsilon_{ij})
= V_j^\top \Sigma V_j + \pi^2 / 3.
\end{aligned}
$$

The **structure matrix** $S$ is defined as the correlation between each item and factor:
$$
\begin{aligned}
S_{jk} &= Cor(Y^*_{ij}, U_{ik}) \\
&= \frac{(V_j^\top \Sigma)_k}{
\sqrt{\Sigma_{kk}} \sqrt{V_j^\top \Sigma V_j + \pi^2 / 3}
}.
\end{aligned}
$$
:::

The following code implements this computation.

```{python}
def compute_structure(V_rot, U):
    U_c = U - U.mean(dim=0, keepdim=True)
    N = U.shape[0]

    # Phi = compute_cov(U_c.T)  # [K, K]
    Phi = torch.corrcoef(U_c.T)  # [K, K]
    
    # logistic error variance
    err_var = math.pi**2 / 3.0
    # numerator = A Phi
    APhi = V_rot @ Phi  # [M, K]
    # denominator: sqrt(a_j^T Phi a_j + sigma^2) for each row
    quad = (V_rot @ Phi * V_rot).sum(dim=1, keepdim=True)  # [M, 1]
    denom_items = torch.sqrt(quad + err_var)               # [M, 1]
    # factor-wise denominator: sqrt(Phi_kk)
    denom_factors = torch.sqrt(torch.diag(Phi)).unsqueeze(0)  # [1, K]
    # standardized loadings
    structure = APhi / (denom_items * denom_factors)  # [M, K]

    assert structure.max() <= 1.0 and structure.min() >= -1.0, "Structure not bounded between -1 and 1"

    return structure, Phi



structure, _ = compute_structure(V_rot_2, U_rot_2)
print(structure.shape)


```

### Clustering Items into Subgroups

With the structure matrix in hand, we can cluster the items into latent subgroups using a Gaussian Mixture Model (GMM). Each cluster represents a group of items sharing similar factor loadings — analogous to “skills” or “themes” within the benchmark.

```{python}

X = structure.detach().cpu().numpy()
models = []
clusters = range(1,10)
for cluster in clusters:


    models.append(GaussianMixture(n_components=cluster, covariance_type="full", n_init=5, random_state=0).fit(X))
# models = [GaussianMixture(n_components=cluster, covariance_type="full", n_init=5, random_state=0).fit(X) for k in ks]
bics = [m.bic(X) for m in models]
best_n_cluster = clusters[int(np.argmin(bics))]
labels = models[int(np.argmin(bics))].predict(X)
print("number of clusters", len(set(labels)))


```
::: {.callout-note title="Interpretation"}
The **Bayesian Information Criterion (BIC)** selects the optimal number of clusters.  
A lower BIC indicates a better trade-off between model fit and complexity, revealing the natural clusters of the benchmark.
::: 


### Visualize the structure matrix and the cluster groups
```{python}
import seaborn as sns
import matplotlib.pyplot as plt
palette = sns.color_palette("colorblind", best_n_cluster)
structure_np = structure.detach().cpu().numpy()
df_plot = pd.DataFrame(structure_np)
df_plot['cluster'] = labels
plt.figure(figsize=(8,6))
sns.scatterplot(data=df_plot, x=0, y=1, hue='cluster', palette=palette)
plt.title("Scatter plot of items in factor space colored by cluster")
plt.xlabel("Factor 1")
plt.ylabel("Factor 2")
plt.legend(title='Cluster')
plt.show()
```



## Quantifying benchmark subgroups correlations
Within each benchmark, we compute inter-construct correlations by averaging
model scores within clusters and correlating these means across models.
Strongly positive inter-cluster correlations indicate overlapping constructs,
while weak or negative correlations suggest that distinct and possibly
conflicting capabilities are being aggregated by the benchmark’s mean score.
This multidimensional pattern explains why two models with identical overall
accuracies may excel on entirely different skill axes.


```{python}

group_mean = []
num_sample = 100
for cluster_id in range(best_n_cluster):
    print("Processing group", cluster_id, "size", sum(labels == cluster_id))
    
    group_mean.append(Y[:,labels == cluster_id].mean(1))
overall_mean = Y.mean(1)
group_mean.append(overall_mean)
M = torch.stack(group_mean)
corr = torch.corrcoef(M)
print("correlation between cluster means",corr)

```

### Discussion On Cluster Groups

![First row shows the structure matrix with group assignments. The second row reports correlations between cluster means and the overall mean, and the third row visualizes the item structure to highlight the group organization](Figures/cluster_FA.png){#fig-cluster_FA width="80%"}


In [@fig-benchmark_auc], the first row shows the structure matrix with group assignments, the second row reports correlations between cluster means and the overall mean, and the third row visualizes the item structure to highlight the group organization. The results show that many benchmarks exhibit a heterogeneous cluster of items whose means are only weakly correlated, while others display clear tension where the two group means are negatively correlated. When this happens, the benchmark-level mean score is neither informative nor accurate about the subgroup score due to heterogeneity. Factor models assign a feature vector to each item (i.e., the structure vector), allowing them to be clustered via a standard clustering algorithm, which helps the reader better interpret the evaluation result that is missed by the mean score. 
<!-- 
We compare two models on MMLU-Pro and show that, while their overall scores are close (53.1% vs. 54.3%), they demonstrate different capability profiles ([@fig-model_profile_mmlu_pro]). This highlights the risk of relying only on mean benchmark scores: two models with the same average may actually specialize in very different skill clusters. 

![The spider plot compares two models on MMLU--Pro reveals distinct capability profiles despite similar overall scores.](Figures/model_profile_mmlu_pro.png){#fig-model_profile_mmlu_pro width="40%"} -->





## Generalization Using 7 Masking Schemes

To evaluate the robustness and transferability of the factor model, we train and test it under **seven masking schemes**, each representing a different notion of generalization.  
These masks determine which parts of the response matrix \(Y\) are visible during training and which are held out for evaluation.

| **Masking Type** | **Train Set** | **Test Set** | **Purpose / Procedure** |
|------------------|---------------|---------------|--------------------------|
| Entry-wise random | 80% random entries | 20% random entries | Interpolation under missing-at-random; no distribution shift |
| Row holdout (random) | 80% of models, all items | 20% of models, all items | Generalization to unseen models; freeze \(V,Z\), estimate \(U\) for test models |
| Row holdout (shifted) | Slice of model population (small → large / old → new) | Disjoint slice | Covariate-shift generalization; test if \(V,Z\) transfer across model generations |
| Column holdout (random) | All models, 80% of items | All models, 20% of items | Generalization to unseen items; freeze \(U\), re-estimate \(V,Z\) |
| Column holdout (shifted) | Train on subset of benchmarks (e.g., MMLU + BBH) | Hold out other benchmarks (e.g., GPQA + MUSR) | Cross-domain transfer of model representations |
| Row–column block (L-mask) | \(R_{tr} \times C_{tr}\) | \(R_{te} \times C_{te}\) | Compositional generalization to unseen model–item combinations |
| Temporal split | Models uploaded before cutoff date | Models uploaded after cutoff date | Temporal generalization; tests how \(V,Z\) transfer over time |

These settings parallel psychometric validation tests in which new examinees, items, or contexts are introduced to probe the invariance of latent constructs.

### Implementation

The following code block illustrates how each masking strategy is generated using helper functions:
Note that in this demo code below for various masking and two stage training are not executed. However, they reflect implementation in the paper.


```{python}
#| eval: false
import torch
import numpy as np

# ==============================
# Helper functions for masking
# ==============================

def random_mask(data_idtor, pct=0.8):
    """
    Entry-wise random masking.
    Randomly selects pct of entries for training; rest for testing.
    """
    train_idtor = torch.bernoulli(data_idtor * pct).int()
    test_idtor = data_idtor.int() - train_idtor
    return train_idtor, test_idtor


def model_mask(data_idtor, pct_models=0.8, exposure_rate=0.3):
    """
    Row holdout (random): hold out unseen models.
    80% of models for training, expose small fraction for unseen ones.
    """
    train_row_mask = torch.bernoulli(torch.ones(data_idtor.shape[0]) * pct_models).bool()
    train_idtor = torch.zeros_like(data_idtor).int()
    # expose full rows for train models
    train_idtor[train_row_mask, :] = data_idtor[train_row_mask, :]
    # expose limited entries for test models
    train_idtor[~train_row_mask, :], _ = random_mask(data_idtor[~train_row_mask, :], pct=exposure_rate)
    test_idtor = data_idtor - train_idtor
    return train_idtor, test_idtor


def item_mask(data_idtor, pct_items=0.8, exposure_rate=0.3):
    """
    Column holdout (random): hold out unseen items.
    80% of items for training, expose small fraction of held-out columns.
    """
    train_col_mask = torch.bernoulli(torch.ones(data_idtor.shape[1]) * pct_items).bool()
    train_idtor = torch.zeros_like(data_idtor).int()
    train_idtor[:, train_col_mask] = data_idtor[:, train_col_mask]
    # limited exposure for held-out items
    train_idtor[:, ~train_col_mask], _ = random_mask(data_idtor[:, ~train_col_mask], pct=exposure_rate)
    test_idtor = data_idtor - train_idtor
    return train_idtor, test_idtor


def L_mask(data_idtor, pct_models=0.8, pct_items=0.8):
    """
    Row–column block (L-mask): compositional generalization.
    Train on subset of models × subset of items.
    """
    train_row_mask = torch.bernoulli(torch.ones(data_idtor.shape[0]) * pct_models).bool()
    train_col_mask = torch.bernoulli(torch.ones(data_idtor.shape[1]) * pct_items).bool()
    train_idtor = torch.zeros_like(data_idtor).int()

    # observed only where both row and column are in training subset
    train_idtor[train_row_mask][:, train_col_mask] = data_idtor[train_row_mask][:, train_col_mask]
    test_idtor = data_idtor - train_idtor

    # remove overlap between train/test domains
    test_idtor[train_row_mask, :] = 0
    test_idtor[:, train_col_mask] = 0
    return train_idtor, test_idtor


# ==============================
# Generate 4 masking schemes
# ==============================

masking_schemes = {
    "rand_rand": lambda: random_mask(data_idtor, pct=0.8),
    "randrow_randrow": lambda: model_mask(data_idtor),
    "randcol_randcol": lambda: item_mask(data_idtor),
    "L_mask": lambda: L_mask(data_idtor),
}

for name, func in masking_schemes.items():
    print(f"Creating mask: {name}")
    train_idtor, test_idtor = func()
    print(f"Train coverage: {train_idtor.sum().item()/data_idtor.sum().item():.2%}")
    print(f"Test coverage:  {test_idtor.sum().item()/data_idtor.sum().item():.2%}")
```



Each mask defines a **train–test exposure pattern** over the model–item matrix.  
For example:

- **Row holdout (random):** unseen models test whether the model can infer new ability vectors $U_i$ with fixed item parameters $(V, Z)$.
- **Column holdout (random):** unseen items probe whether learned latent factors capture transferrable skill dimensions.
- **L-mask:** both models and items are partially unseen, forming a *compositional generalization* challenge.

### Train

To avoid **data contamination**, the factor model uses a *two-stage training procedure* for **row** and **column** masking schemes.  
This ensures that latent parameters learned from the training subset do not leak information from the held-out models or items.

---

#### Row holdout (random): two-stage training

In the **row holdout** setting, the goal is to test generalization to unseen models.  
We divide the response matrix \(Y\) into training and testing subsets along the *model axis*:


$$
Y =
\begin{bmatrix}
Y_{\text{train models}} \\\\[3pt]
Y_{\text{test models}}
\end{bmatrix}
$$


1. **Stage 1 — Train on known models.**  
   Fit the factor model using only the rows corresponding to training models:
   $$
   \min_{U_{\text{train}}, V, Z}
   -\!\!\sum_{(i,j)\in\text{train}} \!
   \log p(Y_{ij}\mid U_i, V_j, Z_j).
   $$
   This learns shared item parameters \(V,Z\).

2. **Stage 2 — Infer unseen models.**  
   Freeze \(V,Z\) from Stage 1, and re-estimate \(U_{\text{test}}\) for the held-out models using their limited revealed responses:
   $$
   \min_{U_{\text{test}}}
   -\!\!\sum_{(i,j)\in\text{test}}
   \log p(Y_{ij}\mid U_i, V_j^{\text{frozen}}, Z_j^{\text{frozen}}).
   $$
   This allows evaluating whether the learned item representations generalize to *new model populations*.

```{python}
#| eval: false
# --- Row holdout training (randrow_randrow) ---
test_row = test_idtor.max(axis=1).values

# Stage 1: Train on known models
model_stage1 = JML_trainer(
    Y_missing[~test_row, :],
    K=factor,
    mask=train_idtor[~test_row, :],
    device="cuda:0",
    is_map=True
)
V_frozen, Z_frozen, _ = standardize_V_Z_U_promax(
    model_stage1.U.cpu(), model_stage1.V.cpu(), model_stage1.Z.cpu()
)

# Stage 2: Estimate U for unseen models
model_stage2 = JML_trainer(
    Y_missing[test_row, :],
    K=factor,
    mask=train_idtor[test_row, :],
    device="cuda:0",
    V_fixed=V_frozen,
    Z_fixed=Z_frozen
)
```

#### Column holdout (random): two-stage training


In the **column holdout** setting, the model must generalize to unseen items (questions).  
We partition the response matrix $Y$ along the *item axis*:

$$
Y = [\, Y_{\text{train items}} \;\; Y_{\text{test items}} \,].
$$

**Stage 1 — Train on known items.**  
Fit the model using only the columns corresponding to training items:

$$
\min_{U, V_{\text{train}}, Z_{\text{train}}}
- \!\! \sum_{(i,j)\in \text{train}}
\log p(Y_{ij}\mid U_i, V_j, Z_j).
$$

**Stage 2 — Infer unseen items.**  
Freeze $U$ from Stage 1, and re-estimate $V_{\text{test}}, Z_{\text{test}}$ for the held-out items:

$$
\min_{V_{\text{test}}, Z_{\text{test}}}
- \!\! \sum_{(i,j)\in \text{test}}
\log p(Y_{ij}\mid U_i^{\text{frozen}}, V_j, Z_j).
$$

This procedure measures the **transferability of model-side representations** to new tasks or benchmarks.

```{python}
#| eval: false
# --- Column holdout training (randcol_randcol) ---

# Identify held-out item columns
test_col = test_idtor.max(axis=0).values

# ===== Stage 1: Train on known items =====
model_stage1 = JML_trainer(
    Y_missing[:, ~test_col],
    K=factor,
    mask=train_idtor[:, ~test_col],
    device="cuda:0",
    is_map=True
)

# Extract and standardize parameters; freeze U for next stage
_, _, U_frozen = standardize_V_Z_U_promax(
    model_stage1.U.cpu(),
    model_stage1.V.cpu(),
    model_stage1.Z.cpu()
)

# ===== Stage 2: Estimate V, Z for unseen items =====
model_stage2 = JML_trainer(
    Y_missing[:, test_col],
    K=factor,
    mask=train_idtor[:, test_col],
    device="cuda:0",
    U_fixed=U_frozen
)
```

### Summary

- **randrow (row holdout):** prevents leakage of model-specific knowledge by freezing \(V, Z\) and re-estimating \(U\).

- **randcol (column holdout):** prevents leakage of item information by freezing \(U\) and re-estimating \(V, Z\).


### Evaluation

For each masking scheme, the factor model is trained (or partially re-trained) under the appropriate two-stage procedure, and **evaluated only on the held-out subsets**—that is, unseen models (`test_row`) or unseen items (`test_col`).  
This ensures that evaluation strictly measures generalization rather than memorization.


```{python}
#| eval: false

from torchmetrics import AUROC

mask_names = ["rand_rand", "randrow_randrow", "randcol_randcol", "L_mask"]
results = []

for mask_name in mask_names:
    print(f"\nEvaluating generalization under {mask_name}")
    train_idtor, test_idtor = masking_schemes[mask_name]()
    test_row = test_idtor.max(axis=1).values
    test_col = test_idtor.max(axis=0).values

    if mask_name == "rand_rand":
        # Single-stage: standard random masking
        model = JML_trainer(Y_missing, K=factor, mask=train_idtor, device="cuda:0", is_map=True)
        P_hat = model().detach().cpu()
        eval_mask = test_idtor.bool()

    elif mask_name == "randrow_randrow":
        # Two-stage: evaluate on held-out models
        model_train = JML_trainer(Y_missing[~test_row, :], K=factor, mask=train_idtor[~test_row, :], device="cuda:0", is_map=True)
        V_frozen, Z_frozen, _ = standardize_V_Z_U_promax(model_train.U.cpu(), model_train.V.cpu(), model_train.Z.cpu())
        model_test = JML_trainer(Y_missing[test_row, :], K=factor, mask=train_idtor[test_row, :], device="cuda:0", V_fixed=V_frozen, Z_fixed=Z_frozen)
        P_hat = model_test().detach().cpu()
        eval_mask = test_idtor[test_row, :].bool()

    elif mask_name == "randcol_randcol":
        # Two-stage: evaluate on held-out items
        model_train = JML_trainer(Y_missing[:, ~test_col], K=factor, mask=train_idtor[:, ~test_col], device="cuda:0", is_map=True)
        _, _, U_frozen = standardize_V_Z_U_promax(model_train.U.cpu(), model_train.V.cpu(), model_train.Z.cpu())
        model_test = JML_trainer(Y_missing[:, test_col], K=factor, mask=train_idtor[:, test_col], device="cuda:0", U_fixed=U_frozen)
        P_hat = model_test().detach().cpu()
        eval_mask = test_idtor[:, test_col].bool()

    elif mask_name == "L_mask":
        # Compositional generalization: unseen models × unseen items
        model = JML_trainer(Y_missing[~test_row, :][:, ~test_col], K=factor, mask=train_idtor[~test_row, :][:, ~test_col], device="cuda:0", is_map=True)
        P_hat = model().detach().cpu()
        eval_mask = test_idtor[test_row, :][:, test_col].bool()

    # Compute AUC on held-out entries
    auroc = AUROC(task="binary")
    auc_score = auroc(P_hat[eval_mask], Y[eval_mask])
    results.append((mask_name, auc_score.item()))

    print(f"AUC[{mask_name}] = {auc_score.item():.3f}")


```

### Results

![The AUC of the factor model on various ranks and datasets. Different panels represent different train-test data partition methods for generalization stress tests, such as generalization to unseen language models or questions.](Figures/auc_all.png){#fig-benchmark_auc width="90%"}


The factor model performance across six benchmarks is shown in [@fig-benchmark_auc]. Best rank varies: some tasks peak at low ranks (e.g., OpenLLM–Math), while others require larger latent spaces. For the best rank across all datasets, the AUC ranges from 92% to 97% for the random mask, indicating an excellent model fit that lays the foundation for further analysis. 



## Adaptive Measurement via Computerized Adaptive Testing (CAT)

Building on the multidimensional factor model, we can design an **adaptive evaluation procedure** that measures model ability efficiently while maintaining reliability.  
Given estimated item parameters $(\hat V, \hat Z)$, we iteratively select the most informative question for a given model (or "test taker") based on the **Fisher Information** criterion.

At step $t$, the Fisher information of item $j$ with respect to the current ability estimate $\hat U_i^{(t)}$ is:

$$
I^{(t)}_{ij} = p^{(t)}_{ij} \left( 1 - p^{(t)}_{ij} \right) \hat V_j \hat V_j^\top, 
\quad p^{(t)}_{ij} = \sigma(\hat V_j^\top \hat U_i^{(t)} + \hat Z_j).
$$

The D-optimal item to administer is the one that maximizes the determinant of the accumulated information:

$$
j^* = \arg\max_j \det\!\left( \sum_{\tau=1}^{t} I^{(\tau)}_{ij} \right).
$$

Once the response $Y_{ij^*}$ is observed, we update the ability estimate $\hat U_i^{(t+1)}$ using a **maximum a posteriori (MAP)** inference step with prior mean $\mu$ and covariance $\Sigma$:

$$
\hat U_i^{(t+1)} = 
\arg\max_U \left[ 
\sum_{\tau=1}^{t} Y_{ij_\tau} (V_{j_\tau}^\top U + Z_{j_\tau})
- \log(1 + e^{V_{j_\tau}^\top U + Z_{j_\tau}}) 
- \tfrac{1}{2}(U - \mu)^\top \Sigma^{-1}(U - \mu)
\right].
$$

The test proceeds until a desired **average reliability threshold** is reached.  
Per-dimension reliability is computed as:

$$
R_k = 1 - \frac{\hat\Sigma_{i,kk}^2}{\operatorname{Var}(\hat U_{k})},
\quad 
\bar{R} = \frac{1}{K} \sum_{k=1}^{K} R_k,
$$

where $\hat\Sigma_i \approx (I^{(1:t)}_i)^{-1}$ is the posterior covariance estimated from the Fisher information.  
Evaluation terminates when $\bar{R} \ge 0.95$ (95% reliability) or when the item budget is depleted.

```{python}
#| eval: false
# Pseudocode for CAT loop
current_theta = theta_mean
answered = []
reliability = 0

while reliability < 0.95:
    # Select next item maximizing Fisher information (D-optimality)
    j_star = select_next_item_mdet(current_theta, answered)
    response = true_response[j_star]
    answered.append((j_star, response))

    # Update ability estimate
    current_U = estimate_U(answered, prior_mean, prior_cov_inv)

    # Compute reliability from posterior covariance
    sem = np.sqrt(np.diag(np.linalg.inv(calculate_fisher_matrix(answered))))
    reliability = 1 - (sem**2 / np.var(theta_population))
```



This adaptive procedure parallels Computerized Adaptive Testing (CAT) in psychometrics, but here applied to AI model evaluation.
It efficiently identifies the ability vector $U_i$ with far fewer items than fixed test forms, maintaining statistical precision while minimizing redundant evaluation.

CAT thus bridges factor-analytic modeling and practical evaluation, transforming benchmarks into dynamic, information-efficient measurement systems for model capabilities.

we randomly sampled 200 held-out LLMs, for each of which we administered items with learned parameters chosen either randomly or adaptively using Fisher information. [@fig-reliability_steps] shows that adaptive testing consistently requires far fewer items than random selection across benchmarks--reducing the number of items required by the baseline by at least 33.9\% and up to 94.3\%. 

![Number of question required to reach average reliability of 0.95 under random vs. adaptive item selection. Adaptive testing consistently requires fewer queries across all benchmarks.](Figures/reliability_steps.png){#fig-reliability_steps width="50%"}



## Implications for measurement
The discovered multidimensional constructs form a basis for **construct-level
evaluation**, where each dimension is measured and reported separately.
This provides finer-grained insight into model behavior, enabling identification
of trade-offs (e.g., reasoning vs. memorization) and informing targeted model
improvement. Moreover, the same latent-factor framework supports adaptive testing:
by selecting items with maximal Fisher information for a given ability estimate,
we can measure each construct efficiently and reliably within limited evaluation
budgets.



